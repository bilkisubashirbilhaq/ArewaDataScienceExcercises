{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Extract all incoming email addresses as a list from the email_exchange_big.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email_addresses(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Use regex to find all email addresses\n",
    "    email_addresses = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', content)\n",
    "    \n",
    "    return email_addresses\n",
    "\n",
    "# Example usage\n",
    "email_addresses = extract_email_addresses('email_exchange_big.txt')\n",
    "print(email_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the most common words in the English language. Call the name of your function find_most_common_words, it will take two parameters - a string or a file and a positive integer, indicating the number of words. Your function will return an array of tuples in descending order. Check the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test', 3), ('this', 2), ('is', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def find_most_common_words(text_or_file, num_words):\n",
    "    # Check if the input is a file path or a string\n",
    "    try:\n",
    "        with open(text_or_file, 'r') as file:\n",
    "            text = file.read()\n",
    "    except FileNotFoundError:\n",
    "        text = text_or_file\n",
    "    \n",
    "    # Use regex to find all words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get the most common words\n",
    "    most_common_words = word_counts.most_common(num_words)\n",
    "    \n",
    "    return most_common_words\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a test. This test is only a test.\"\n",
    "print(find_most_common_words(text, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use the function, find_most_frequent_words to find: a) The ten most frequent words used in Obama's speech b) The ten most frequent words used in Michelle's speech c) The ten most frequent words used in Trump's speech d) The ten most frequent words used in Melina's speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama's speech: [('the', 129), ('and', 113), ('of', 81), ('to', 70), ('our', 67), ('we', 62), ('that', 50), ('a', 48), ('is', 36), ('in', 25)]\n",
      "Michelle's speech: [('and', 96), ('the', 85), ('to', 84), ('that', 50), ('of', 46), ('a', 41), ('he', 37), ('in', 36), ('my', 28), ('i', 28)]\n",
      "Trump's speech: [('the', 65), ('and', 59), ('we', 44), ('will', 40), ('of', 38), ('to', 32), ('our', 30), ('is', 20), ('america', 17), ('for', 13)]\n",
      "Melania's speech: [('and', 77), ('to', 55), ('the', 52), ('is', 29), ('i', 28), ('for', 27), ('of', 25), ('that', 24), ('a', 22), ('you', 21)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming the speeches are stored in text files\n",
    "obama_speech = 'obama_speech.txt'\n",
    "michelle_speech = 'michelle_speech.txt'\n",
    "trump_speech = 'trump_speech.txt'\n",
    "melania_speech = 'melania_speech.txt'\n",
    "\n",
    "# Find the ten most frequent words in each speech\n",
    "obama_common_words = find_most_common_words(obama_speech, 10)\n",
    "michelle_common_words = find_most_common_words(michelle_speech, 10)\n",
    "trump_common_words = find_most_common_words(trump_speech, 10)\n",
    "melania_common_words = find_most_common_words(melania_speech, 10)\n",
    "\n",
    "print(\"Obama's speech:\", obama_common_words)\n",
    "print(\"Michelle's speech:\", michelle_common_words)\n",
    "print(\"Trump's speech:\", trump_common_words)\n",
    "print(\"Melania's speech:\", melania_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a python application that checks similarity between two texts. It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. For instance check the similarity between the transcripts of Michelle's and Melina's speech. You may need a couple of functions, function to clean the text(clean_text), function to remove support words(remove_support_words) and finally to check the similarity(check_text_similarity). List of stop words are in the data directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Michelle's and Melania's speeches: 0.2665131829446272\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text).lower()\n",
    "    return text\n",
    "\n",
    "def remove_support_words(text):\n",
    "    # Remove stop words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def check_text_similarity(text1, text2):\n",
    "    # Clean and remove support words from both texts\n",
    "    text1 = remove_support_words(clean_text(text1))\n",
    "    text2 = remove_support_words(clean_text(text2))\n",
    "    \n",
    "    # Vectorize the texts\n",
    "    vectorizer = TfidfVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity[0][1]\n",
    "\n",
    "# Example usage\n",
    "with open(michelle_speech, 'r') as file:\n",
    "    michelle_text = file.read()\n",
    "\n",
    "with open(melania_speech, 'r') as file:\n",
    "    melania_text = file.read()\n",
    "\n",
    "similarity_score = check_text_similarity(michelle_text, melania_text)\n",
    "print(f\"Similarity between Michelle's and Melania's speeches: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Find the 10 most repeated words in the romeo_and_juliet.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo and Juliet: [('romeo_and_juliet', 1), ('txt', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Find the 10 most repeated words in the romeo_and_juliet.txt\n",
    "romeo_and_juliet = 'romeo_and_juliet.txt'\n",
    "romeo_and_juliet_common_words = find_most_common_words(romeo_and_juliet, 10)\n",
    "\n",
    "print(\"Romeo and Juliet:\", romeo_and_juliet_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Read the hacker news csv file and find out: a) Count the number of lines containing python or Python b) Count the number lines containing JavaScript, javascript or Javascript c) Count the number lines containing Java and not JavaScript\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m hacker_news_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhacker_news.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Count the number of lines containing 'python' or 'Python'\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m python_count \u001b[38;5;241m=\u001b[39m \u001b[43mhacker_news_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[Pp]ython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Count the number of lines containing 'JavaScript', 'javascript' or 'Javascript'\u001b[39;00m\n\u001b[0;32m     10\u001b[0m javascript_count \u001b[38;5;241m=\u001b[39m hacker_news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[Jj]ava[Ss]cript\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the hacker news CSV file\n",
    "hacker_news_df = pd.read_csv('hacker_news.csv')\n",
    "\n",
    "# Count the number of lines containing 'python' or 'Python'\n",
    "python_count = hacker_news_df['title'].str.contains(r'\\b[Pp]ython\\b', regex=True).sum()\n",
    "\n",
    "# Count the number of lines containing 'JavaScript', 'javascript' or 'Javascript'\n",
    "javascript_count = hacker_news_df['title'].str.contains(r'\\b[Jj]ava[Ss]cript\\b', regex=True).sum()\n",
    "\n",
    "# Count the number of lines containing 'Java' and not 'JavaScript'\n",
    "java_count = hacker_news_df['title'].str.contains(r'\\b[Jj]ava\\b', regex=True) & ~hacker_news_df['title'].str.contains(r'\\b[Jj]ava[Ss]cript\\b', regex=True)\n",
    "java_count = java_count.sum()\n",
    "\n",
    "print(f\"Number of lines containing 'python' or 'Python': {python_count}\")\n",
    "print(f\"Number of lines containing 'JavaScript', 'javascript' or 'Javascript': {javascript_count}\")\n",
    "print(f\"Number of lines containing 'Java' and not 'JavaScript': {java_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
